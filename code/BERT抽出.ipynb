{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers fugashi ipadic"
      ],
      "metadata": {
        "id": "rvnPNj2tXVq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers fugashi ipadic unidic-lite"
      ],
      "metadata": {
        "id": "TcZLS0ToXXoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "\n",
        "# 日本語BERT（BERT系）モデル\n",
        "model_name = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# GPU があれば使う\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def split_sentences(text: str):\n",
        "    \"\"\"\n",
        "    ざっくり句点で文分割（必要ならあとで賢くしてOK）\n",
        "    \"\"\"\n",
        "    sents = re.split(r\"(?<=。|\\!|\\?)\", text)\n",
        "    return [s.strip() for s in sents if s.strip()]\n",
        "\n",
        "def summarize(text: str, target_chars=120):\n",
        "    \"\"\"\n",
        "    text : 入力テキスト\n",
        "    target_chars : だいたいこのくらいの文字数がほしい（目安）\n",
        "    BERTで抽出型要約（重要そうな文を抜き出す）\n",
        "    \"\"\"\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return \"\"\n",
        "\n",
        "    # 各文を BERT でベクトルに変換（[CLS]トークンを使う）\n",
        "    inputs = tokenizer(\n",
        "        sentences,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # [batch, seq_len, hidden] -> [batch, hidden]\n",
        "        sent_embeds = outputs.last_hidden_state[:, 0, :]  # CLS\n",
        "\n",
        "    # テキスト全体の代表ベクトル（文ベクトルの平均）\n",
        "    doc_embed = sent_embeds.mean(dim=0, keepdim=True)  # [1, hidden]\n",
        "\n",
        "    # 各文とのコサイン類似度を計算\n",
        "    sim = F.cosine_similarity(sent_embeds, doc_embed.repeat(len(sentences), 1))\n",
        "\n",
        "    # スコアが高い順に文インデックスを並べる\n",
        "    sorted_idx = torch.argsort(sim, descending=True).tolist()\n",
        "\n",
        "    # 文字数制限に近づけるように、重要な文から順に追加\n",
        "    selected = []\n",
        "    current_len = 0\n",
        "    max_chars = int(target_chars * 1.3)  # 少し余裕\n",
        "\n",
        "    for idx in sorted_idx:\n",
        "        sent = sentences[idx]\n",
        "        if current_len + len(sent) > max_chars and selected:\n",
        "            continue\n",
        "        selected.append((idx, sent))\n",
        "        current_len += len(sent)\n",
        "        if current_len >= target_chars:\n",
        "            break\n",
        "\n",
        "    # 元の順番に並び替え\n",
        "    selected.sort(key=lambda x: x[0])\n",
        "    summary = \" \".join(s for _, s in selected)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# === 実行部分 ===\n",
        "text = input(\"要約したいテキストを貼ってください：\\n\")\n",
        "target = int(input(\"だいたい何文字くらいで要約してほしいですか？：\"))\n",
        "\n",
        "print(\"\\n--- 要約結果 (BERT抽出型) ---\\n\")\n",
        "result = summarize(text, target_chars=target)\n",
        "print(result)\n",
        "print(\"\\n【実際の文字数】：\", len(result))"
      ],
      "metadata": {
        "id": "PorbBGuJXZav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}